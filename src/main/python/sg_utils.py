# CONSTANTS

import codecs
import csv
import urllib2
import sys


# path to raw soure urls file, created by running WmfExtractEnhancer on
# the urls extracted by running get_labs_urls on tool labs
import cStringIO

PATH_SOURCE_URLS = '../../../dat/source_urls.tsv'

# path to result of running get_langs on the scraped sources
PATH_URL_LANGS = '../../../dat/url_langs.tsv'

# Path to raw whois file, from dave
PATH_WHOIS_RAW = '../../../dat/whois_results5.txt'

# Path to file generated by running build_url_to_whois.py
PATH_URL_WHOIS = '../../../dat/url_whois.tsv'
PATH_URL_WHOIS_TEST = '../../../dat/url_whois_test.tsv'

# Path to interesting urls (those we use in our analysis)
PATH_URL_INTERESTING = '../../../dat/interesting_urls.txt'

# Number of times each url is referenced
# Generated by running build_url_counts.py
PATH_URL_COUNTS = '../../../dat/url_counts.tsv'

# Country info from geonames
PATH_COUNTRY_INFO = '../../../dat/countryInfo.txt'

# Distances between countries
PATH_COUNTRY_DISTANCES = '../../../dat/country_distances.tsv'

# Path for prior distribution of countries, built by running build_overall_dists.py
PATH_COUNTRY_PRIOR = '../../../dat/country_priors.txt'

# Wikidata domain locations, built by running WikidataLocator.java
PATH_WIKIDATA_DOMAIN_LOCATIONS = '../../../dat/domain_wikidata_locations.tsv'

# Wikidata url locations, built by running build_wikidata_locations.py
PATH_WIKIDATA_URL_LOCATIONS = '../../../dat/url_wikidata_locations.tsv'

# Final result file
PATH_URL_RESULT = '../../../dat/url_locations.tsv'

# Binary versions of large data structures cached for efficiency
PATH_DAO_CACHE = '../../../dat/dao-cache.bin'

# Path to the 2012 data
PATH_2012 = '../../../dat/2012_test.tsv'

# Path to the ISO 639 mapping file
PATH_ISO_639 = '../../../dat/iso-639-3.tab.txt'

# Path to ethnologue languages to countries file. Must be purchased.
PATH_ETHNOLOGUE = '../../../dat/ethnologue_17_global_dataset/Table_of_LICs.tab'

# Alternative country names to codes
PATH_COUNTRY_NAMES = '../../../dat/country_names.tsv'

# Final coded dataset
PATH_CODED_URL_COUNTRIES = '../../../dat/final_coded_url_countries.tsv'

GENERIC_TLDS = set('ad,as,bz,cc,cd,co,dj,fm,io,la,me,ms,nu,sc,sr,su,tv,tk,ws,int'.split(','))

def warn(message):
    sys.stderr.write(message + '\n')


# The encoded reader from io is faster but only available in Python >= 2.6
try:
    import io
    enc_open = io.open
except:
    enc_open = codecs.open


def sg_open(path, mode='r', encoding='utf-8'):
    return enc_open(path, mode, encoding=encoding)

class UTF8Recoder:
    """
    Iterator that reads an encoded stream and reencodes the input to UTF-8
    """
    def __init__(self, f, encoding):
        self.reader = codecs.getreader(encoding)(f)

    def __iter__(self):
        return self

    def next(self):
        return self.reader.next().encode("utf-8")

class UnicodeDictReader:
    """
    A CSV reader which will iterate over lines in the CSV file "f",
    which is encoded in the given encoding.
    """

    def __init__(self, f, dialect=csv.excel, encoding="utf-8", **kwds):
        self.f = f
        f = UTF8Recoder(f, encoding)
        self.reader = csv.reader(f, dialect=dialect, **kwds)
        self.fieldnames = self.reader.next()

    def next(self):
        row = self.reader.next()
        vals = [unicode(s, "utf-8") for s in row]
        return dict((self.fieldnames[x], vals[x]) for x in range(len(self.fieldnames)))

    def __iter__(self):
        return self

    def close(self):
        self.f.close()

class UnicodeWriter:
    """
    A CSV writer which will write rows to CSV file "f",
    which is encoded in the given encoding.
    """

    def __init__(self, f, dialect=csv.excel, encoding="utf-8", **kwds):
        # Redirect output to a queue
        self.queue = cStringIO.StringIO()
        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
        self.stream = f
        self.encoder = codecs.getincrementalencoder(encoding)()

    def writerow(self, row):
        self.writer.writerow([unicode(s).encode("utf-8") for s in row])
        # Fetch UTF-8 output from the queue ...
        data = self.queue.getvalue()
        data = data.decode("utf-8")
        # write to the target stream
        self.stream.write(data)
        # empty queue
        self.queue.truncate(0)

    def writerows(self, rows):
        for row in rows:
            self.writerow(row)

    def close(self):
        self.stream.close()

class UnicodeDictWriter(csv.DictWriter, object):
    def __init__(self, f, fieldnames, restval="", extrasaction="raise", dialect="excel", *args, **kwds):
        super(UnicodeDictWriter, self).__init__(f, fieldnames, restval="", extrasaction="raise", dialect="excel", *args, **kwds)
        self.writer = UnicodeWriter(f, dialect, **kwds)

    def writeheader(self):
        self.writer.writerow(self.fieldnames)

    def close(self):
        self.writer.close()

def sg_open_csvr(path, delimiter=None, encoding='utf-8'):
    if not delimiter:
        if path.endswith('tsv'):
            delimiter = '\t'
        elif path.endswith('csv'):
            delimiter = ','
        else:
            raise Exception('no recognized delimiter for ' + path)

    f = sg_open(path, encoding=encoding)
    return UnicodeDictReader(f, delimiter=delimiter)

def sg_open_csvw(path, fields, delimiter=None):
    if not delimiter:
        if path.endswith('tsv'):
            delimiter = '\t'
        elif path.endswith('csv'):
            delimiter = ','
        else:
            raise Exception('no recognized delimiter for ' + path)

    f = sg_open(path, 'w')
    w = UnicodeDictWriter(f, fields, delimiter=delimiter)
    w.writeheader()
    return w

def url2host(url):
    return urllib2.urlparse.urlparse(url).netloc