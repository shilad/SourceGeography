# Order of running steps:
#
# 1. Download source_urls.tsv
# 2. Download whois_results2.tsv
# 3. Build whois url file: python build_url_to_whois.py
# 4. Build wikidata url file: python build_wikidata_locations.py
# 5. Build prior distribution: python build_country_priors.py
# 6. (Perhaps) Rerun step 5 to get a more accurate estimate.
# 7. Run the inferrer:


# path to raw soure urls file, created by running WmfExtractEnhancer on
# the urls extracted by running get_labs_urls on tool labs
PATH_SOURCE_URLS = '../../../dat/source_urls.tsv'

# path to result of running get_langs on the scraped sources
PATH_URL_LANGS = '../../../dat/url_langs.tsv'

# Path to raw whois file, from dave
PATH_WHOIS_RAW = '../../../dat/whois_results2.txt'

# Path to file generated by running build_url_to_whois.py
PATH_URL_WHOIS = '../../../dat/url_whois.tsv'
PATH_URL_WHOIS_TEST = '../../../dat/url_whois_test.tsv'


# Country info from geonames
PATH_COUNTRY_INFO = '../../../dat/countryInfo.txt'

# Path for prior distribution of countries, built by running build_overall_dists.py
PATH_COUNTRY_PRIOR = '../../../dat/country_priors.txt'

# Wikidata domain locations, built by running WikidataLocator.java
PATH_WIKIDATA_DOMAIN_LOCATIONS = '../../../dat/domain_wikidata_locations.tsv'

# Wikidata url locations, built by running build_wikidata_locations.py
PATH_WIKIDATA_URL_LOCATIONS = '../../../dat/url_wikidata_locations.tsv'

# Final result file
PATH_URL_RESULT = '../../../dat/url_locations.tsv'

